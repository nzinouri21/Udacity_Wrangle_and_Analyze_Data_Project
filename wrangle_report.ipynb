{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Wrangling Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling for this project consisted of the three main wrangling acts:\n",
    "1. Gathering data\n",
    "2. Assessing data\n",
    "3. Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were three main pieces of data that had to be gathered for this project:\n",
    "\n",
    "1. `The WeRateDogs Twitter archive`. This data was provided by Udacity as csv file: twitter_archive_enhanced.csv, we just had to read it into a dataframe\n",
    "\n",
    "2. `The tweet image predictions data`, which contains predictions for breed of dog using a neural network model for prediction. This file (image_predictions.tsv) is hosted on Udacity's servers and had to be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "3. Lastly, for each tweet, `retweet count and favorite (\"like\") count` was required. Using the tweet IDs in the WeRateDogs Twitter archive, we had to query the Twitter API for each tweet's JSON data using Python's Tweepy library. Then store tweet's entire set of JSON data in a file called tweet_json.txt file. And finally read this .txt file line by line into a pandas DataFrame with tweet ID, retweet count, and favorite count. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a variety of Python functions such as `.info` `.head()` `.tail()` `.duplicated()` `value_counts()` and more, all three pieces of data were programmatically assessed for both quality and tidiness issues. The following is the summary of assessment:\n",
    "\n",
    "**Quality issues:**\n",
    "1. 181 rows are actually retweets and not original tweets (retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp columns are not null for these records).\n",
    "2. 78 rows are similarly replies and not original tweets (in_reply_to_status_id, in_reply_to_user_id fields are not null for these).\n",
    "3. There are 59 tweets that are missing expanded URL field.\n",
    "4. source column can be cleaned up to be more readable. There are only 4 main categories before the final \"<\\a>\": Twitter for iPhone, Vine - Make a Scene, Twitter Web Client, and TweetDeck.\n",
    "5. The timestamp column is in object (string) format instead of datetime.\n",
    "6. In several columns such as name, doggo, floofer, pupper, puppo, it is hard to identify missing values because they are None instead of null.\n",
    "7. name column has 745 values as None which is most likely missing values. And there are other values that are not names such as 'a', 'the' for 55 rows.\n",
    "8. We know from the project description that in WeRateDogs, denominator should be 10. However, there are 20 rows with denominator greater than 10.\n",
    "9. 25 tweet ids at the time this code was run, were either deleted or set to private and are missing. They are missing favorite and retweet count. This will be addressed after all dataframes are joined.\n",
    "10. There are some extreme numerators which are either false or group dog photos. Better to limit to a maximum of 15.\n",
    "\n",
    "**Tidiness issues:**\n",
    "1. Columns retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id, in_reply_to_user_id are not relevant to original tweets.\n",
    "2. There are 4 columns for dog stages (doggo, floofer, pupper, puppo). These are all different values for one variable and should be in one column.\n",
    "3. Columns p1, p2, p3 can be refined into one prediction to more meaningful. Same thing for other columns such as p1_conf or p1_dog better as well.\n",
    "4. All three dataframes should be merged into one dataframe that contains all relevant information\n",
    "\n",
    "On top of these issues that were identified during the assessment, more problems were found during the clean-up process. For example, after using melt to convert dog life stages ( 'doggo', 'floofer', 'pupper' and 'puppo') into one column, resulted in a dataframe with four times more rows as the original one. This turned out to be because of the rows with None for all columns AKA no life stage. A workaround was required and the initial cleaning strategy did not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `define`, `code`, `test` approach, all the above issues were cleaned. THe three pieces of data were combined, saved in a csv and later used for analysis. A combination of built-in Python functions and methods as well as user written functions were used for cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
